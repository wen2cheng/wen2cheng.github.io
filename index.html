<!--
 * @Author: wencheng
 * @Date: 2021-08-17 12:28:36
 * @LastEditors: wencheng
 * @LastEditTime: 2021-08-17 14:24:45
 * @Description: 
-->
<html>
  <head>
    <meta charset="UTF-8">
    <title>Audio samples from "KeSpeech: A Multipurpose Speech Dataset of Mandarin and Its Eight Subdialects"</title>
    <link rel="stylesheet" type="text/css" href="../../stylesheet.css"/>
    <link rel="shortcut icon" href="../../images/taco.png">
  </head>
  <body>
    <article>
      <header>
        <h1>Audio samples from "KeSpeech: A Multipurpose Speech Dataset of Mandarin and Its Eight Subdialects"</h1>
      </header>
    </article>
    <div><b>Paper: </b><a href="https://arxiv.org/abs/2010.09275">arXiv</a></div>
    <div><b>Authors:</b> Zhiyuan Tang, -, -, -, -, Wei Zou, Xiangang L</div>
    <div><b>Abstract:</b> This paper introduces an open source speech dataset, KeSpeech, which involves the most spoken Chinese dialect \emph{Mandarin} and its 8 subdialects across 36 cities in China.
      Specifically, the dataset provides multiple supervisions of transcription, speaker identity and subdialect type to support a variety of speech tasks, such as speech recognition, speaker recognition, dialect identification and their multi-task learning.
      Moreover, some of the text samples are parallel recorded with both standard Chinese and a subdialect, allowing for learning dialect style conversion, and there is certain interval between the two phases of recording for most of the speakers, allowing for the study of time invariance in speaker recognition.
      Also, we describe the design, creation and license of the dataset, and based on some baseline experiments including speech recognition and speaker recognition, we do some analysis to show the challenges involved in those tasks and inspirations the dataset may provide.<br/><br/></div>
    <h2>目标说话人Ground Truth</h2>
    <h2>说话人A</h2>
    <div>Audio samples of both the parallel and non-parallel voice conversion (VC) models trained on the DiDiSpeech corpus are provided here. In the rest of this section, the source and target audio, which has been separated from the training data, is the speech samples recorded from source and target speakers respectively. The converted audio is the speech samples converted from the source audio in the same line by using our VC models.</div>
    <h3></h3>
    <h3>1、普通话</h3>
    <blockquote>
      <table>
        <tr>
          <td align=center width=200></td><td align=center width=200>Source audio</td><td align=center width=200>Target audio</td><td align=center width=200>Converted audio</td>
        </tr>
        <tr>
          <td style="white-space:nowrap" align=center width=300>Inter-gender sample (Female)</td>
          <td><audio controls><source src="vc/chengdu_dialect_1024426-to-qingdao_accent_1007609/eval/chengdu_dialect-to-qingdao_accent.01.wav"></audio></td>
          <td><audio controls><source src="vc/chengdu_dialect_1024426-to-qingdao_accent_1007609/eval/chengdu_dialect-to-qingdao_accent.01.wav"></audio></td>
          <td><audio controls><source src="vc/chengdu_dialect_1024426-to-qingdao_accent_1007609/eval/chengdu_dialect-to-qingdao_accent.01.wav"></audio></td>
        </tr>
        <tr height="10px"></tr>
        <tr>
          <td style="white-space:nowrap" align=center width=300> </td>
          <td><audio controls><source src="vc/chengdu_dialect_1024426-to-qingdao_accent_1007609/eval/chengdu_dialect-to-qingdao_accent.01.wav"></audio></td>
          <td><audio controls><source src="vc/chengdu_dialect_1024426-to-qingdao_accent_1007609/eval/chengdu_dialect-to-qingdao_accent.01.wav"></audio></td>
          <td><audio controls><source src="vc/chengdu_dialect_1024426-to-qingdao_accent_1007609/eval/chengdu_dialect-to-qingdao_accent.01.wav"></audio></td>
        </tr>
        <tr height="10px"></tr>
      </table>
    </blockquote>
    <h3>2、方言</h3>
    <blockquote>
      <table>
        <tr>
          <td align=center width=200></td><td align=center width=200>Source audio</td><td align=center width=200>Target audio</td><td align=center width=200>Converted audio</td>
        </tr>
        <tr>
          <td style="white-space:nowrap" align=center width=300>Inter-gender sample (Female)</td>
          <td><audio controls><source src="vc/chengdu_dialect_1024426-to-qingdao_accent_1007609/eval/chengdu_dialect-to-qingdao_accent.01.wav"></audio></td>
          <td><audio controls><source src="vc/chengdu_dialect_1024426-to-qingdao_accent_1007609/eval/chengdu_dialect-to-qingdao_accent.01.wav"></audio></td>
          <td><audio controls><source src="vc/chengdu_dialect_1024426-to-qingdao_accent_1007609/eval/chengdu_dialect-to-qingdao_accent.01.wav"></audio></td>
        </tr>
        <tr height="10px"></tr>
        <tr>
          <td style="white-space:nowrap" align=center width=300> </td>
          <td><audio controls><source src="vc/chengdu_dialect_1024426-to-qingdao_accent_1007609/eval/chengdu_dialect-to-qingdao_accent.01.wav"></audio></td>
          <td><audio controls><source src="vc/chengdu_dialect_1024426-to-qingdao_accent_1007609/eval/chengdu_dialect-to-qingdao_accent.01.wav"></audio></td>
          <td><audio controls><source src="vc/chengdu_dialect_1024426-to-qingdao_accent_1007609/eval/chengdu_dialect-to-qingdao_accent.01.wav"></audio></td>
        </tr>
        <tr height="10px"></tr>
      </table>
    </blockquote>
    <h2>说话人B</h2>
  </body>
</html>
